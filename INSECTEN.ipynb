{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install icrawler\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfKapjfX_Bs-",
        "outputId": "b2224cf2-d0d8-4062-95d9-bf24bfffe905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting icrawler\n",
            "  Downloading icrawler-0.6.10-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from icrawler) (4.13.4)\n",
            "Collecting bs4 (from icrawler)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from icrawler) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from icrawler) (11.2.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from icrawler) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from icrawler) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from icrawler) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->icrawler) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->icrawler) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (2025.4.26)\n",
            "Downloading icrawler-0.6.10-py3-none-any.whl (36 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4, icrawler\n",
            "Successfully installed bs4-0.0.2 icrawler-0.6.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN0nrPMr-vC8",
        "outputId": "62062e8a-a1de-4e40-8da5-0d24e2e4ed1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Downloading images for: cricket\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 404, file http://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Gryllus_campestris_MHNT.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.australiangeographic.com.au/wp-content/uploads/2023/02/shutterstock_676240501-1800x1227.jpg\n",
            "ERROR:downloader:Exception caught when downloading file https://cricketinsect.com/wp-content/uploads/2019/08/What-Are-Crickets.jpg, error: HTTPSConnectionPool(host='cricketinsect.com', port=443): Max retries exceeded with url: /wp-content/uploads/2019/08/What-Are-Crickets.jpg (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1016)'))), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https://cricketinsect.com/wp-content/uploads/2019/08/What-Are-Crickets.jpg, error: HTTPSConnectionPool(host='cricketinsect.com', port=443): Max retries exceeded with url: /wp-content/uploads/2019/08/What-Are-Crickets.jpg (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1016)'))), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https://cricketinsect.com/wp-content/uploads/2019/08/What-Are-Crickets.jpg, error: HTTPSConnectionPool(host='cricketinsect.com', port=443): Max retries exceeded with url: /wp-content/uploads/2019/08/What-Are-Crickets.jpg (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1016)'))), remaining retry times: 0\n",
            "ERROR:downloader:Response status code 403, file https://thebugmaster.com/wp-content/uploads/2018/08/how-to-prevent-crickets.jpg\n",
            "ERROR:downloader:Exception caught when downloading file https://www.worldatlas.com/r/w1200-q80/upload/43/d6/e4/shutterstock-743534308.jpg, error: HTTPSConnectionPool(host='www.worldatlas.com', port=443): Max retries exceeded with url: /r/w1200-q80/upload/43/d6/e4/shutterstock-743534308.jpg (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)'))), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https://www.worldatlas.com/r/w1200-q80/upload/43/d6/e4/shutterstock-743534308.jpg, error: HTTPSConnectionPool(host='www.worldatlas.com', port=443): Max retries exceeded with url: /r/w1200-q80/upload/43/d6/e4/shutterstock-743534308.jpg (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)'))), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https://www.worldatlas.com/r/w1200-q80/upload/43/d6/e4/shutterstock-743534308.jpg, error: HTTPSConnectionPool(host='www.worldatlas.com', port=443): Max retries exceeded with url: /r/w1200-q80/upload/43/d6/e4/shutterstock-743534308.jpg (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1016)'))), remaining retry times: 0\n",
            "ERROR:downloader:Response status code 400, file https://media.gettyimages.com/id/93390134/photo/cricket-in-singapore-botanic-garden.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.gettyimages.com/id/460713739/photo/bush-cricket.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.texasstandard.org/wp-content/uploads/2017/09/3111255081_5907ca02cc_o-e1506350956355.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Downloading images for: grasshopper\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/d9/b7/photo-155202.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/27/88/grasshopper_insect_bug_nature_green_grass_macro_wild-672714.jpg\n",
            "ERROR:downloader:Response status code 403, file https://coolwallpapers.me/picsup/3027502-close-up_grasshopper_insect_macro.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Downloading images for: termite\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://www.flapest.com/wp-content/uploads/2021/02/dampwood-termite-1.jpg\n",
            "ERROR:downloader:Response status code 403, file https://noosapest.com/wp-content/uploads/2020/01/termites-3367350_1920-1.jpg\n",
            "ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Isoptera.jpg\n",
            "ERROR:downloader:Response status code 403, file https://johnsonpestcontrol.com/wp-content/uploads/2020/11/what-termite-looks-like.jpg\n",
            "ERROR:downloader:Response status code 403, file https://cdn.rentokil.com/content/global/images/desktop/main_termite-banner.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.aaipest.com/wp-content/uploads/2018/03/AAI-Pest-Control-10-Termite-Facts.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Downloading images for: cockroach\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://www.flick.com.au/wp-content/uploads/Why-is-unhealthy-to-have-cockroaches-in-your-home.jpg\n",
            "ERROR:downloader:Response status code 403, file https://cramerpestcontrol.com/wp-content/uploads/2019/02/cockroach-bug.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.callnorthwest.com/wp-content/uploads/2020/04/bigstock-American-Cockroach-48025229-1-scaled.jpg\n",
            "ERROR:downloader:Response status code 403, file https://inman-murphy.com/wp-content/uploads/German-cockroaches-1.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.westernpest.com/wp-content/uploads/Oriental-Cockroach-5-scaled.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Downloading images for: mosquito\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 404, file https://upload.wikimedia.org/wikipedia/commons/thumb/d/dc/Mosquito_2007-2.jpg\n",
            "ERROR:downloader:Response status code 403, file https://thebugmaster.com/wp-content/uploads/2019/05/Anopheles-mosquito-.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.gettyimages.com/id/174688732/photo/mosquito.jpg\n",
            "ERROR:downloader:Response status code 403, file https://c.pxhere.com/photos/23/d7/crane_fly_insect_late_summer_animal_daddy_long_legs-692871.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Downloading images for: housefly\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 400, file https://media.istockphoto.com/id/1400023278/photo/musca-domestica-housefly-insect.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧹 Cleaning dataset...\n",
            " - cricket: removed 0 bad images\n",
            " - grasshopper: removed 0 bad images\n",
            " - termite: removed 0 bad images\n",
            " - cockroach: removed 0 bad images\n",
            " - mosquito: removed 0 bad images\n",
            " - housefly: removed 0 bad images\n",
            "\n",
            "✅ Cleaning complete. Total removed: 0\n"
          ]
        }
      ],
      "source": [
        "from icrawler.builtin import BingImageCrawler\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Step 1: Define insect classes and edibility (for potential future use)\n",
        "insects = {\n",
        "    'cricket': 'edible',\n",
        "    'grasshopper': 'edible',\n",
        "    'termite': 'edible',\n",
        "    'cockroach': 'non_edible',\n",
        "    'mosquito': 'non_edible',\n",
        "    'housefly': 'non_edible'\n",
        "}\n",
        "\n",
        "# Step 2: Create base dataset directory\n",
        "base_dir = 'insect_dataset'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Step 3: Download images\n",
        "images_per_class = 100\n",
        "for insect in insects:\n",
        "    insect_dir = os.path.join(base_dir, insect)\n",
        "    os.makedirs(insect_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"⬇️ Downloading images for: {insect}\")\n",
        "    crawler = BingImageCrawler(storage={'root_dir': insect_dir})\n",
        "    crawler.crawl(keyword=f\"{insect} insect\", max_num=images_per_class)\n",
        "\n",
        "# Step 4: Clean dataset (remove corrupted and low-quality images)\n",
        "def clean_directory(directory, min_width=100, min_height=100):\n",
        "    removed = 0\n",
        "    for filename in os.listdir(directory):\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        try:\n",
        "            with Image.open(filepath) as img:\n",
        "                img.verify()  # Check if it's a readable image\n",
        "                img = Image.open(filepath)\n",
        "                if img.size[0] < min_width or img.size[1] < min_height:\n",
        "                    os.remove(filepath)\n",
        "                    removed += 1\n",
        "        except Exception:\n",
        "            os.remove(filepath)\n",
        "            removed += 1\n",
        "    return removed\n",
        "\n",
        "# Apply cleaning to each class folder\n",
        "print(\"\\n🧹 Cleaning dataset...\")\n",
        "total_removed = 0\n",
        "for insect in insects:\n",
        "    dir_path = os.path.join(base_dir, insect)\n",
        "    removed = clean_directory(dir_path)\n",
        "    total_removed += removed\n",
        "    print(f\" - {insect}: removed {removed} bad images\")\n",
        "\n",
        "print(f\"\\n✅ Cleaning complete. Total removed: {total_removed}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def clean_dataset_images(directory, valid_exts={'.jpg', '.jpeg', '.png', '.bmp', '.gif'}):\n",
        "    removed_files = 0\n",
        "    for subdir, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            filepath = os.path.join(subdir, file)\n",
        "            ext = os.path.splitext(file)[1].lower()\n",
        "\n",
        "            # Remove files without valid image extensions\n",
        "            if ext not in valid_exts:\n",
        "                print(f\"Removing non-image file: {filepath}\")\n",
        "                os.remove(filepath)\n",
        "                removed_files += 1\n",
        "                continue\n",
        "\n",
        "            # Try to open and verify image with PIL\n",
        "            try:\n",
        "                with Image.open(filepath) as img:\n",
        "                    img.verify()\n",
        "            except Exception as e:\n",
        "                print(f\"Removing corrupted image file: {filepath} ({e})\")\n",
        "                os.remove(filepath)\n",
        "                removed_files += 1\n",
        "                continue\n",
        "\n",
        "            # Additional check: load image with tensorflow (decoding)\n",
        "            import tensorflow as tf\n",
        "            try:\n",
        "                img_raw = tf.io.read_file(filepath)\n",
        "                _ = tf.io.decode_image(img_raw)\n",
        "            except Exception as e:\n",
        "                print(f\"Removing unreadable image file (TF decode error): {filepath} ({e})\")\n",
        "                os.remove(filepath)\n",
        "                removed_files += 1\n",
        "\n",
        "    print(f\"🧹 Removed {removed_files} invalid image files.\")\n",
        "\n",
        "# Use it before loading dataset\n",
        "clean_dataset_images('insect_dataset')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBSZfc8KDXZ0",
        "outputId": "85afa997-09d8-474b-cb7a-9c56baf5908e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing unreadable image file (TF decode error): insect_dataset/termite/000030.jpg ({{function_node __wrapped__DecodeImage_device_/job:localhost/replica:0/task:0/device:CPU:0}} Unknown image file format. One of JPEG, PNG, GIF, BMP required. [Op:DecodeImage] name: )\n",
            "Removing unreadable image file (TF decode error): insect_dataset/grasshopper/000036.jpg ({{function_node __wrapped__DecodeImage_device_/job:localhost/replica:0/task:0/device:CPU:0}} Unknown image file format. One of JPEG, PNG, GIF, BMP required. [Op:DecodeImage] name: )\n",
            "🧹 Removed 2 invalid image files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, RandomFlip, RandomRotation, RandomZoom\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Params\n",
        "data_dir = 'insect_dataset'\n",
        "img_height, img_width = 180, 180\n",
        "batch_size = 32\n",
        "epochs_initial = 10\n",
        "epochs_finetune = 5\n",
        "seed = 123\n",
        "\n",
        "# Dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=seed,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "val_ds = image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=seed,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(\"Class names:\", class_names)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(AUTOTUNE)\n",
        "\n",
        "# Augmentation\n",
        "data_augmentation = Sequential([\n",
        "    RandomFlip(\"horizontal\"),\n",
        "    RandomRotation(0.1),\n",
        "    RandomZoom(0.1)\n",
        "])\n",
        "\n",
        "# Base model\n",
        "base_model = MobileNetV2(input_shape=(img_height, img_width, 3), include_top=False, weights=\"imagenet\")\n",
        "base_model.trainable = False\n",
        "\n",
        "# Multi-class model\n",
        "model = Sequential([\n",
        "    data_augmentation,\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(len(class_names), activation='softmax')  # Multi-class output\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "print(\"\\n🔧 Training base model (frozen)...\\n\")\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=epochs_initial)\n",
        "\n",
        "# Fine-tune\n",
        "print(\"\\n🎯 Fine-tuning model...\\n\")\n",
        "base_model.trainable = True\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=epochs_finetune)\n",
        "\n",
        "# Save model\n",
        "model.save(\"insect_species_classifier.h5\")\n",
        "print(\"✅ Model saved as 'insect_species_classifier.h5'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdTIzBhnAeah",
        "outputId": "86164605-02c4-4a1f-8f6f-078c50d1676c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 435 files belonging to 6 classes.\n",
            "Using 348 files for training.\n",
            "Found 435 files belonging to 6 classes.\n",
            "Using 87 files for validation.\n",
            "Class names: ['cockroach', 'cricket', 'grasshopper', 'housefly', 'mosquito', 'termite']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-3706960008>:52: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(input_shape=(img_height, img_width, 3), include_top=False, weights=\"imagenet\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Training base model (frozen)...\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 2s/step - accuracy: 0.2001 - loss: 2.1966 - val_accuracy: 0.2529 - val_loss: 1.9332\n",
            "Epoch 2/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.3574 - loss: 1.6671 - val_accuracy: 0.2759 - val_loss: 1.7277\n",
            "Epoch 3/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.4116 - loss: 1.5356 - val_accuracy: 0.3793 - val_loss: 1.5868\n",
            "Epoch 4/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.4786 - loss: 1.3461 - val_accuracy: 0.4023 - val_loss: 1.5491\n",
            "Epoch 5/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.4959 - loss: 1.2304 - val_accuracy: 0.3793 - val_loss: 1.4947\n",
            "Epoch 6/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.5473 - loss: 1.1939 - val_accuracy: 0.4138 - val_loss: 1.4874\n",
            "Epoch 7/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.5706 - loss: 1.1445 - val_accuracy: 0.4253 - val_loss: 1.4593\n",
            "Epoch 8/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.5326 - loss: 1.1432 - val_accuracy: 0.4368 - val_loss: 1.4271\n",
            "Epoch 9/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.5982 - loss: 1.0978 - val_accuracy: 0.4368 - val_loss: 1.4985\n",
            "Epoch 10/10\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.6546 - loss: 0.9985 - val_accuracy: 0.4828 - val_loss: 1.4019\n",
            "\n",
            "🎯 Fine-tuning model...\n",
            "\n",
            "Epoch 1/5\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 5s/step - accuracy: 0.2663 - loss: 2.4176 - val_accuracy: 0.4713 - val_loss: 1.3768\n",
            "Epoch 2/5\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 5s/step - accuracy: 0.2524 - loss: 2.3310 - val_accuracy: 0.4943 - val_loss: 1.4400\n",
            "Epoch 3/5\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 5s/step - accuracy: 0.3360 - loss: 1.8763 - val_accuracy: 0.5172 - val_loss: 1.5006\n",
            "Epoch 4/5\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 5s/step - accuracy: 0.4504 - loss: 1.5647 - val_accuracy: 0.5517 - val_loss: 1.5279\n",
            "Epoch 5/5\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 5s/step - accuracy: 0.5190 - loss: 1.3998 - val_accuracy: 0.4828 - val_loss: 1.5451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved as 'insect_species_classifier.h5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cn7AleCHmns",
        "outputId": "50318a6c-544a-40f4-9478-57f3a4883be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.45.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load model\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    return tf.keras.models.load_model('insect_species_classifier.h5')\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# Insect classes from your training\n",
        "class_names = ['cockroach', 'cricket', 'grasshopper', 'housefly', 'mosquito', 'termite']\n",
        "\n",
        "# Map insect to edibility\n",
        "edibility_map = {\n",
        "    'cricket': '🟢 Edible',\n",
        "    'grasshopper': '🟢 Edible',\n",
        "    'termite': '🟢 Edible',\n",
        "    'cockroach': '🔴 Non-Edible',\n",
        "    'mosquito': '🔴 Non-Edible',\n",
        "    'housefly': '🔴 Non-Edible'\n",
        "}\n",
        "\n",
        "st.title(\"🦋Isaac InsectEN🐞\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload an insect image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    img = Image.open(uploaded_file).convert(\"RGB\").resize((180, 180))\n",
        "    st.image(img, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "    img_array = np.array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    prediction = model.predict(img_array)[0]\n",
        "    predicted_index = np.argmax(prediction)\n",
        "    predicted_class = class_names[predicted_index]\n",
        "    confidence = prediction[predicted_index]\n",
        "    edibility = edibility_map.get(predicted_class, \"⚪ Unknown\")\n",
        "\n",
        "    st.subheader(f\"Prediction: {predicted_class.capitalize()}\")\n",
        "    st.write(f\"Edibility: {edibility}\")\n",
        "    st.write(f\"Confidence Score: {confidence:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqYYyL4iGYLr",
        "outputId": "dc4c42da-e887-4dd7-fd73-916cf1c0740e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-14 06:27:05.458 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-14 06:27:05.599 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-06-14 06:27:05.600 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-14 06:27:05.601 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-14 06:27:06.105 Thread 'Thread-49': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-14 06:27:06.107 Thread 'Thread-49': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "2025-06-14 06:27:06.406 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-14 06:27:06.408 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-14 06:27:06.410 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-14 06:27:06.411 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-14 06:27:06.412 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-14 06:27:06.413 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-14 06:27:06.414 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-14 06:27:06.415 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-06-14 06:27:06.416 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "code = '''import streamlit as st\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load model\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    return tf.keras.models.load_model('insect_species_classifier.h5')\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# Insect classes from your training\n",
        "class_names = ['cockroach', 'cricket', 'grasshopper', 'housefly', 'mosquito', 'termite']\n",
        "\n",
        "# Map insect to edibility\n",
        "edibility_map = {\n",
        "    'cricket': '🟢 Edible',\n",
        "    'grasshopper': '🟢 Edible',\n",
        "    'termite': '🟢 Edible',\n",
        "    'cockroach': '🔴 Non-Edible',\n",
        "    'mosquito': '🔴 Non-Edible',\n",
        "    'housefly': '🔴 Non-Edible'\n",
        "}\n",
        "\n",
        "st.title(\"🦋Isaac InsectEN🐞\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload an insect image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    img = Image.open(uploaded_file).convert(\"RGB\").resize((180, 180))\n",
        "    st.image(img, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "    img_array = np.array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    prediction = model.predict(img_array)[0]\n",
        "    predicted_index = np.argmax(prediction)\n",
        "    predicted_class = class_names[predicted_index]\n",
        "    confidence = prediction[predicted_index]\n",
        "    edibility = edibility_map.get(predicted_class, \"⚪ Unknown\")\n",
        "\n",
        "    st.subheader(f\"Prediction: {predicted_class.capitalize()}\")\n",
        "    st.write(f\"Edibility: {edibility}\")\n",
        "    st.write(f\"Confidence Score: {confidence:.2f}\")\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# Save to app.py\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# Trigger download\n",
        "files.download(\"app.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_Vm5nrYsIEL7",
        "outputId": "f331610d-82bc-43c1-cb7a-4023fead2142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_faa952ca-b1d4-49dd-9650-5ce68fd9da63\", \"app.py\", 1399)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}